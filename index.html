<html>
  <head>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

    <link rel="shortcut icon" href="images/icon.ico" />
    <style type="text/css">
      body {
        background-color: #f5f9ff;
      }

      /* Hide both math displays initially, will display based on JS detection */
      .mathjax-mobile,
      .mathml-non-mobile {
        display: none;
      }

      /* Show the MathML content by default on non-mobile devices */
      .show-mathml .mathml-non-mobile {
        display: block;
      }
      .show-mathjax .mathjax-mobile {
        display: block;
      }

      .content-margin-container {
        display: flex;
        width: 100%; /* Ensure the container is full width */
        justify-content: left; /* Horizontally centers the children in the container */
        align-items: center; /* Vertically centers the children in the container */
      }
      .main-content-block {
        width: 70%; /* Change this percentage as needed */
        max-width: 1100px; /* Optional: Maximum width */
        background-color: #fff;
        border-left: 1px solid #ddd;
        border-right: 1px solid #ddd;
        padding: 8px 8px 8px 8px;
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light",
          "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
      }
      .margin-left-block {
        font-size: 14px;
        width: 15%; /* Change this percentage as needed */
        max-width: 130px; /* Optional: Maximum width */
        position: relative;
        margin-left: 10px;
        text-align: left;
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light",
          "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        padding: 5px;
      }
      .margin-right-block {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light",
          "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-size: 14px;
        width: 25%; /* Change this percentage as needed */
        max-width: 256px; /* Optional: Maximum width */
        position: relative;
        text-align: left;
        padding: 10px; /* Optional: Adds padding inside the caption */
      }

      img {
        max-width: 100%; /* Make sure it fits inside the container */
        height: auto;
        display: block;
        margin: auto;
      }
      .my-video {
        max-width: 100%; /* Make sure it fits inside the container */
        height: auto;
        display: block;
        margin: auto;
      }
      /* Hide both video displays initially, will display based on JS detection */
      .vid-mobile,
      .vid-non-mobile {
        display: none;
      }

      /* Show the video content by default on non-mobile devices */
      .show-vid-mobile .vid-mobile {
        display: block;
      }
      .show-vid-non-mobile .vid-non-mobile {
        display: block;
      }

      a:link,
      a:visited {
        color: #0e7862; /*#1367a7;*/
        text-decoration: none;
      }
      a:hover {
        color: #24b597; /*#208799;*/
      }

      h1 {
        font-size: 18px;
        margin-top: 4px;
        margin-bottom: 10px;
      }

      h2 {
        font-size: 16px;
        margin-top: 4px;
        margin-bottom: 8px;
      }

      table.header {
        font-weight: 300;
        font-size: 17px;
        flex-grow: 1;
        width: 70%;
        max-width: calc(
          100% - 290px
        ); /* Adjust according to the width of .paper-code-tab */
      }
      table td,
      table td * {
        vertical-align: middle;
        position: relative;
      }
      table.paper-code-tab {
        flex-shrink: 0;
        margin-left: 8px;
        margin-top: 8px;
        padding: 0px 0px 0px 8px;
        width: 290px;
        height: 150px;
      }

      .layered-paper {
        /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow: 0px 0px 1px 1px rgba(0, 0, 0, 0.35),
          /* The top layer shadow */ 5px 5px 0 0px #fff,
          /* The second layer */ 5px 5px 1px 1px rgba(0, 0, 0, 0.35),
          /* The second layer shadow */ 10px 10px 0 0px #fff,
          /* The third layer */ 10px 10px 1px 1px rgba(0, 0, 0, 0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
      }

      hr {
        height: 1px; /* Sets the height of the line to 1 pixel */
        border: none; /* Removes the default border */
        background-color: #ddd; /* Sets the line color to black */
      }

      div.hypothesis {
        width: 80%;
        background-color: #eee;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
        font-family: Courier;
        font-size: 18px;
        text-align: center;
        margin: auto;
        padding: 16px 16px 16px 16px;
      }

      div.citation {
        font-size: 0.8em;
        background-color: #fff;
        padding: 10px;
        height: 200px;
      }

      .fade-in-inline {
        position: absolute;
        text-align: center;
        margin: auto;
        -webkit-mask-image: linear-gradient(
          to right,
          transparent 0%,
          transparent 40%,
          black 50%,
          black 90%,
          transparent 100%
        );
        mask-image: linear-gradient(
          to right,
          transparent 0%,
          transparent 40%,
          black 50%,
          black 90%,
          transparent 100%
        );
        -webkit-mask-size: 8000% 100%;
        mask-size: 8000% 100%;
        animation-name: sweepMask;
        animation-duration: 4s;
        animation-iteration-count: infinite;
        animation-timing-function: linear;
        animation-delay: -1s;
      }

      .fade-in2-inline {
        animation-delay: 1s;
      }

      .inline-div {
        position: relative;
        display: inline-block; /* Makes both the div and paragraph inline-block elements */
        vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
        width: 50px; /* Optional: Adds space between the div and the paragraph */
      }
    </style>

    <title>The Platonic Representation Hypothesis</title>
    <meta
      property="og:title"
      content="The Platonic Representation Hypothesis"
    />
    <meta charset="UTF-8" />
  </head>

  <body>
    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <table class="header" align="left">
          <tr>
            <td colspan="4">
              <span
                style="
                  font-size: 32px;
                  font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */
                "
                >Multimodal Scene Representation Learning for Spatial and
                Temporal Understanding in Video</span
              >
            </td>
          </tr>
          <tr>
            <td align="left">
              <span style="font-size: 17px"
                ><a href="your_website">Deniz Erus</a></span
              >
            </td>
            <td align="left">
              <span style="font-size: 17px"
                ><a href="your_partner's_website">Yijiang Liu</a></span
              >
            </td>
            <td align="left">
              <span style="font-size: 17px"
                ><a href="your_partner's_website">Qilmeg Doudatcz</a></span
              >
            </td>
          </tr>

          <tr>
            <td colspan="4" align="left">
              <span style="font-size: 18px">Final project for 6.7960, MIT</span>
            </td>
          </tr>
        </table>
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="intro">
      <div class="margin-left-block">
        <!-- table of contents here -->
        <div style="position: fixed; max-width: inherit; top: max(20%, 120px)">
          <b style="font-size: 16px">Outline</b><br /><br />
          <a href="#intro">Introduction</a><br /><br />
          <a href="#related_work">Related Work</a><br /><br />
          <a href="#model_architecture">Model Architecture</a><br /><br />
          <a href="#data">Data and Training Setup</a><br /><br />
          <a href="#results">Results and Discussion</a><br /><br />
          <a href="#discussion">Discussion</a><br /><br />
          <a href="#implications_and_limitations"
            >Implications and Limitations</a
          ><br /><br />
          <a href="#citations">References</a><br /><br />
        </div>
      </div>
      <div class="main-content-block">
        <!--You can embed an image like this:-->
        <img src="./images/Teaser.png" width="512px" />
      </div>
      <div class="margin-right-block">Teasor image.</div>
    </div>

    <div class="content-margin-container" id="intro">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Introduction</h1>
        In films and short videos with narrative content, editing is a common
        tool used by directors to highlight key moments and guide the viewer’s
        attention. However, compared with the real physical world, editing
        weakens the continuity of time and space in the “story world”. A
        character may instantly appear in a far-away location in the next shot,
        and long actions may be compressed or skipped entirely. Filmmakers use
        these media to guide the viewer’s attention and construct a directed
        encounter with space, yet architects do not assume that visitors move
        through environments with such fixed viewpoints. This raises an
        important question. Can a model learn a representation of video that
        reflects the identity of a space across many views, lighting conditions,
        and points in time?<br /><br />

        Our project addresses this question by learning multimodal scene
        representations that can tell when different clips refer to the same
        physical environment. Rather than building a full continuity-checking
        system, for the scope of this project, we focus on how video and text
        features organize scenes in an embedding space and how this structure
        changes under multimodal supervision.<br /><br />

        We combine visual embeddings from a VideoMAE encoder with text-based
        features from Qwen2-VL and study how these signals shape the geometry of
        the video embedding space. We use low-dimensional projections and
        similarity matrices to see how scenes organize by location and how they
        recur across an entire film. This is a key step in identifying
        continuity concerns such as lighting inconsistencies or shifts in
        temporal order.<br /><br />

        Within this goal, we explore how much spatial structure is already
        present in a self-supervised video model and how much additional signal
        emerges through multimodal supervision. To do this, we compare three
        settings on the same videos:<br /><br />

        <ul>
          <li>
            <b>Unsupervised baseline</b>: treat VideoMAE as a frozen feature
            extractor and project segment embeddings into 2D using principal
            component analysis (PCA). We then inspect whether the resulting
            geometry implicitly groups scenes by semantic locations such as
            kitchen, living room, or exterior.
          </li>
          <li>
            <b>Teacher supervised variant</b>: uuse a stronger multimodal model,
            Qwen2-VL, as a teacher to assign soft scene labels, and train a
            lightweight classifier head on top of VideoMAE to predict these
            labels from video alone.
          </li>
          <li>
            <b>Text guided semantic distance</b>: embed scene metadata with a
            language model to obtain text similarities between segments, and use
            these similarities as soft weights in a contrastive loss that
            fine-tunes the VideoMAE embeddings toward a more location-aware
            space.
          </li>
        </ul>

        By contrasting these methods, we examine the degree to which a
        self-supervised video model can recover spatial scene categories without
        human labels and how much its representation improves when guided by
        semantic structure from an external teacher.<br /><br />
      </div>
      <div class="margin-right-block">
        Margin note for introduction section.
      </div>
    </div>

    <div class="content-margin-container" id="related_work">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Related Works</h1>
        <h2>Video representation learning</h2>
        Recent work in video understanding has shifted from convolution based
        models toward transformer based architectures that process spacetime
        information more directly. VideoMAE extends masked autoencoding to video
        by removing a very high portion of spacetime patches during training and
        asking the encoder to reconstruct the missing content. Let the video be
        represented as a sequence of patch tokens x ∈ ℝT × N × D. A masking
        operator M selects a small visible subset, and the encoder E processes
        only these tokens to produce a latent representation z = E(M(x)). The
        decoder D then attempts to reconstruct the original patches x̂ = D(z).
        This forces the encoder to learn stable spatial cues, global structure,
        and temporal relationships rather than action specific features. These
        properties make VideoMAE suitable for identifying consistent spatial
        identity across scenes.<br /><br />

        <h2>Vision language modeling</h2>
        Language based models have become essential in video understanding,
        since narrative cues can often clarify spatial or temporal context that
        is not visually explicit. Qwen2-VL provides strong language
        representations and can operate in vision language settings that support
        reasoning over frames and subtitles. In this project, Qwen2-VL is used
        to convert video segments into dense text embeddings. These embeddings
        provide contextual signals about location, time, or narrative intent,
        which stabilizes scene identity when visual content varies due to
        lighting or framing.<br /><br />

        <h2>Multimodal fusion for scene understanding</h2>
        Combining visual and language information has become common in tasks
        such as retrieval, grounding, and cross modal alignment. Multimodal
        fusion allows models to integrate appearance features with narrative
        cues that do not appear in the raw frames. In this project, the
        normalized visual embedding v from VideoMAE and the subtitle embedding t
        from Qwen are concatenated to form a joint representation f = [v, t].
        This fused representation captures both spatial structure and narrative
        meaning. Although multimodal fusion is well established in video
        question answering and video search, it has not been widely applied to
        identifying recurring architectural spaces across scenes.<br /><br />

        In practice, the main multimodal interaction in our experiments happens
        through the contrastive loss described in Section 3.3, where
        text-derived similarities act as soft weights that pull or push pairs of
        visual embeddings. This lets the language model act as an external model
        that shapes the geometry of the video embedding space.<br /><br />
      </div>

      <div class="margin-right-block" style="transform: translate(0%, -100%)">
        <!-- you can move the margin notes up and down with translate -->
        Interestingly, Plato also asked if X does Y, in
        <a href="#ref_1">[1]</a>.
      </div>
    </div>

    <div class="content-margin-container" id="model_architecture">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Model Architecture</h1>
        Our model builds upon a VideoMAE-based visual backbone and extends it
        with a scene-level embedding head and a text-guided contrastive learning
        module. The architecture is designed to combine visual understanding,
        scene segmentation, and world-model reasoning within a unified
        framework. Thus, the proposed architecture for semantic distance
        understanding contains three components: Video Understanding
        Backbone(VideoMAE) + Scene Embedding Head + LLM-based Semantic Distance
        Module (Contrastive Layer).<br /><br />

        <h2>Video Understanding Backbone</h2>
        The base of our system is the pretrained MCG-NJU/videomae-base
        transformer encoder. We load a fine-tuned checkpoint that has already
        been trained on the FineVideo dataset for video understanding tasks. To
        adapt this backbone for scene-level semantic modeling, we support two
        kinds of supervisory signals: Frame-level labels, where each frame
        receives an individual label; and Scene-level shared labels, where all
        frames within a timestamp interval share one scene label. These two
        settings allow the model to shift from coarse video understanding toward
        the more structured task of scene segmentation.<br /><br />

        During fine-tuning, only the last K layers of VideoMAE’s transformer
        encoder (K = 2–4) are unfrozen. This preserves low-level visual
        representations while allowing high-level scene semantics to adapt to
        the downstream objective.<br /><br />

        <h2>Scene Embedding Head (MLP)</h2>
        To produce a scene-level representation, we append a lightweight MLP
        head to the output CLS token: MLP (“Linear”→"ReLU"→"Linear"). The MLP
        compresses the high-dimensional VideoMAE features into a compact
        embedding space in which scene relationships can be reasoned about. This
        embedding space is used both for segmentation tasks and for the semantic
        distance learning described below.<br /><br />

        <h2>Semantic Distance (LLM based) Contrastive training</h2>
        While VideoMAE provides strong visual features, it lacks world knowledge
        needed for higher-level scene reasoning.Film editing often omits
        intermediate transitions because humans can infer continuity from
        common-sense reasoning, and some videos take place in fictional
        universes where everyday expectations do not apply. This requires
        grounding in the actual visual content rather than generic knowledge. To
        address both limitations, we introduce a language-model-powered semantic
        distance module that uses text as an external world model.<br /><br />

        For each scene, we extract metadata descriptions (scene description,
        props, context) and encode them using a large language model to obtain a
        normalized embedding <i>t</i><sub>i</sub> for scene <i>i</i>. Semantic
        relatedness between two scenes <i>i</i> and <i>j</i> is measured as
        cosine similarity<br /><br />

        <center>
          <i>s</i><sub>ij</sub> = ( <i>t</i><sub>i</sub> · <i>t</i
          ><sub>j</sub> ) / ( &#8741;<i>t</i><sub>i</sub>&#8741; &#8741;<i>t</i
          ><sub>j</sub>&#8741; )
        </center>
        <br />

        These embeddings provide semantic relationships that reflect narrative
        continuity, spatial logic, and object-level reasoning.<br /><br />

        We construct soft supervision weights from <i>s</i><sub>ij</sub>, where
        <i>w</i><sup>pos</sup><sub>ij</sub> controls how strongly we pull a pair
        together and <i>w</i><sup>neg</sup><sub>ij</sub> controls how strongly
        we push it apart. We experiment with two implementations for
        <i>f</i> and <i>g</i>:<br /><br />

        Linear mapping:<br /><br />
        <center>
          <i>w</i><sup>pos</sup><sub>ij</sub> = max( <i>s</i><sub>ij</sub>, 0
          ),&nbsp;&nbsp; <i>w</i><sup>neg</sup><sub>ij</sub> = max( &#8722;<i
            >s</i
          ><sub>ij</sub>, 0 )
        </center>
        <br />

        Squared mapping:<br /><br />
        <center>
          <i>w</i><sup>pos</sup><sub>ij</sub> = ( max( <i>s</i><sub>ij</sub>, 0
          ) )<sup>2</sup>,&nbsp;&nbsp; <i>w</i><sup>neg</sup><sub>ij</sub> = (
          max( &#8722;<i>s</i><sub>ij</sub>, 0 ) )<sup>2</sup>
        </center>
        <br />

        In both cases, high text similarity leads to a strong pull, low
        similarity leads to a strong push, and mid-range similarities receive
        relatively small weights. Squaring enhances this effect by emphasizing
        very similar or very dissimilar pairs.<br /><br />

        To achieve this, first, given the video embeddings <i>v</i
        ><sub>i</sub> from the MLP head, we normalize them:<br /><br />

        <center>
          &#770;<i>v</i><sub>i</sub> = <i>v</i><sub>i</sub> / &#8741;<i>v</i
          ><sub>i</sub>&#8741;
        </center>
        <br />

        and compute video-space similarity:<br /><br />

        <center>
          <i>c</i><sub>ij</sub> = &#770;<i>v</i><sub>i</sub> · &#770;<i>v</i
          ><sub>j</sub>
        </center>
        <br />

        Loss of Positive term (pulling):<br /><br />

        <center>
          <i>L</i><sub>pos</sub> = &#8721;<sub>i,j</sub> <i>w</i><sup>pos</sup
          ><sub>ij</sub> ( 1 &#8722; <i>c</i><sub>ij</sub> )
        </center>
        <br />

        Loss of Negative term (pushing):<br /><br />

        <center>
          <i>L</i><sub>neg</sub> = &#8721;<sub>i,j</sub> <i>w</i><sup>neg</sup
          ><sub>ij</sub> <i>c</i><sub>ij</sub>
        </center>
        <br />

        And then we have the final objective:<br /><br />

        <center>
          <i>L</i> = <i>L</i><sub>pos</sub> + <i>L</i><sub>neg</sub>.
        </center>
        <br /><br />

        This module forces the video embedding space to align with the semantic
        topology implied by the LLM, while still remaining grounded in actual
        visual evidence. In this model architecture, both video and language
        have complementary roles. A key motivation for this architecture is that
        video-based and language-based reasoning possess complementary
        strengths:<br /><br />
        VideoMAE compensates for LLM limitations: 1. Fictional or stylized
        worlds violate real-world logic; 2. Visual cues reveal spatial layout,
        lighting, actor identity, etc.<br /><br />
        LLM compensates for VideoMAE limitations:1. Editing creates
        discontinuities humans resolve through world knowledge; 2. Similar
        visual scenes may have very different narrative functions.<br /><br />
        By coupling the two with contrastive learning, the model learns an
        embedding space that reflects both visual relations and
        narrative/world-level semantic similarity. Thus, the final
        representation is capable of supporting fine-grained scene segmentation
        and semantic distance reasoning within longer video sequences.<br /><br />

        Figure: Scene distance of original VideoMAE model before training (The
        blue line shows the interpolated semantic distance, and the orange line
        shows visual distance. The red horizontal line is the semantic
        threshold. For LLM, semantic distance fluctuates strongly and often
        crosses the threshold, while for the original VideoMAE model, visual
        distance stays near zero with only small bumps.)<br /><br />

        Figure: Semantic similarity matrix (Almost all visual distances are
        extremely small, clustered below about 0.02. This indicates that
        VideoMAE sees the majority of clips as visually very similar. The
        spatial layout and low level appearance of the footage remain stable
        across most of the timeline.)
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="data">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Data</h1>
        Discuss the data used in your project, including sources and
        preprocessing.
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="results">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Results</h1>
        <strong>Dataset</strong>: We use the FineVideo dataset from HuggingFace,
        a large-scale collection of short video clips paired with detailed
        metadata. Each entry contains: raw video frames (mp4), structured scene
        annotations, object and prop descriptions, timestamps identifying scene
        boundaries, and contextual descriptions of each clip. This dataset is
        well-suited for training both visual scene classifiers and semantic
        video embedding models, because every scene is accompanied by rich
        natural-language descriptions. <br /><br />

        <strong>Preprocessing and Scene Extraction:</strong>
        FineVideo organizes videos at the segment level and already has assigned
        labels, therefore we can treat each unique timestamp as one scene:<br /><br />

        <ul>
          <li>
            All frames under the same timestamp (sharing one label) are treated
            as belonging to the same semantic scene.
          </li>
          <li>
            We extract relevant text fields from the metadata provided in the
            dataset (scene.description, props.name, content_title,
            contextualRelevance)
          </li>
          <li>
            These text snippets are cleaned and concatenated into a single scene
            description.
          </li>
          <li>
            We compute text embeddings using a pretrained language model and
            store with each scene sample.
          </li>
          <li>
            These embeddings later provide the semantic similarity supervision
            for contrastive training.
          </li>
        </ul>

        We randomly stream and store 3000 videos, covering 29628 scene
        segments.<br /><br />

        <strong>Baseline head training:</strong> We begin with the pretrained
        VideoMAE-Base backbone and add a lightweight fully connected head to
        produce scene-level embeddings. In this stage the backbone is frozen,
        and only the head is trained. We use AdamW with a learning rate of
        10^{-4} for the head and weight decay 10^{-4}. This gives us a visual
        baseline representation for downstream analysis and for the
        teacher-supervised classifier.<br /><br />

        <strong>Text-guided contrastive fine-tuning:</strong> After establishing
        a visual baseline and training several models with it, we introduce
        semantic supervision from text embeddings using our contrastive loss. At
        this stage, we partially unfreeze the last 2–4 transformer layers (we
        used K = 2 in early experiments, K = 4 in final experiments), the new
        learning rates for backbone (unfrozen layers) are 5*10^-5, and 5*10^-4
        for MLP head. During this stage, the video embeddings are trained to
        reflect the semantic distances implied by the textual descriptions.
        Scenes that are close in text space receive stronger “pull” weights, and
        scenes that are far apart receive stronger “push” weights.<br /><br />

        <strong>Teacher-supervised variant:</strong> To add explicit semantic
        structure, we also experiment with a teacher-supervised classifier. For
        each segment, we prompt Qwen2-VL with a short video clip and ask it to
        assign a location label such as “kitchen”, “living room”, or “outside”.
        These labels serve as targets for a linear classifier head on top of the
        scene embeddings. We keep the VideoMAE backbone frozen and train the
        classifier with cross-entropy loss. Comparing this teacher-supervised
        classifier to the unsupervised PCA baseline lets us qualitatively assess
        how much additional structure we gain from multimodal supervision by
        inspecting the PCA projections before and after training.<br /><br />
      </div>
      <div class="margin-right-block"></div>
    </div>
  </body>

  <div class="content-margin-container" id="discussion">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h1>Results and Discussion</h1>
      <h2>PCA Analysis of scene embeddings</h2>
      Discuss the implications of your results, any limitations of your
      approach, and summarize your findings with proposed future work.<br /><br />

      <h2>Text-guided contrastive fine-tuning</h2>
      Show qualitative results such as images or videos generated by your model.
      <br /><br />
    </div>
    <div class="margin-right-block"></div>
  </div>

  <div class="content-margin-container" id="implications_and_limitations">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h1>Implications and limitations</h1>
      Let's end with some discussion of the implications and limitations.

      <h1>Template Section</h1>
      Now let's write some math!<br />
      <center>
        <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mrow>
            <mrow>
              <mo>&#x2202;</mo>
              <mi>y</mi>
            </mrow>
            <mo>/</mo>
            <mrow>
              <mo>&#x2202;</mo>
              <mi>x</mi>
            </mrow>
          </mrow>
          <mo>=</mo>
          <mi>x</mi>
        </math>
      </center>
      <br />
      It's probably best to ask an LLM to help do the web formatting for math.
      You can tell it "convert this latex equation into MathML: $$\frac{\partial
      dy}{\partial dx} = x$$" But it took me a few tries. So, if you get
      frustrated, you can embed an image of the equation, or use other packages
      for rendering equations on webpages.
    </div>
    <div class="margin-right-block"></div>
  </div>

  <div class="content-margin-container">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h1>Another section</h1>
      In this section we embed a video:
      <video class="my-video" loop autoplay muted style="width: 725px">
        <source src="./images/mtsh.mp4" type="video/mp4" />
      </video>
    </div>
    <div class="margin-right-block">A caption for the video could go here.</div>
  </div>

  <div class="content-margin-container" id="citations">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <div class="citation" id="references" style="height: auto">
        <br />
        <span style="font-size: 16px">References:</span><br /><br />
        <a id="ref_1"></a>[1]
        <a href="https://github.com/MCG-NJU/VideoMAE"
          >VideoMAE: Masked Autoencoders are Data-Efficient Learners for
          Self-Supervised Video Pre-Training</a
        >, Zhan Tong and Yibing Song and Jue Wang and Limin Wang, 2022<br /><br />
        <a id="ref_2"></a>[2] <a href="">A Human-Level AGI</a>, OpenAI, 2025<br /><br />
        <a id="ref_2"></a>[2] <a href="">A Human-Level AGI</a>, OpenAI, 2025<br /><br />
        <a id="ref_2"></a>[2] <a href="">A Human-Level AGI</a>, OpenAI, 2025<br /><br />
        <a id="ref_2"></a>[2] <a href="">A Human-Level AGI</a>, OpenAI, 2025<br /><br />
        <a id="ref_2"></a>[2] <a href="">A Human-Level AGI</a>, OpenAI, 2025<br /><br />
      </div>
    </div>
    <div class="margin-right-block">
      <!-- margin notes for reference block here -->
    </div>
  </div>

  <p id="wordCount"></p>
</html>

<script>
  function countWords(text) {
    return text.trim().split(/\s+/).length;
  }

  const content = document.body.innerText;
  const totalWords = countWords(content);

  document.getElementById("wordCount").innerText = "Word count: " + totalWords;
</script>
