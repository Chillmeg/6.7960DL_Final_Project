<html>
  <head>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

    <link rel="shortcut icon" href="images/icon.ico" />
    <style type="text/css">
      body {
        background-color: #f5f9ff;
      }

      /* Hide both math displays initially, will display based on JS detection */
      .mathjax-mobile,
      .mathml-non-mobile {
        display: none;
      }

      /* Show the MathML content by default on non-mobile devices */
      .show-mathml .mathml-non-mobile {
        display: block;
      }
      .show-mathjax .mathjax-mobile {
        display: block;
      }

      .content-margin-container {
        display: flex;
        width: 100%; /* Ensure the container is full width */
        justify-content: left; /* Horizontally centers the children in the container */
        align-items: center; /* Vertically centers the children in the container */
      }
      .main-content-block {
        width: 70%; /* Change this percentage as needed */
        max-width: 1100px; /* Optional: Maximum width */
        background-color: #fff;
        border-left: 1px solid #ddd;
        border-right: 1px solid #ddd;
        padding: 8px 8px 8px 8px;
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light",
          "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
      }
      .margin-left-block {
        font-size: 14px;
        width: 15%; /* Change this percentage as needed */
        max-width: 130px; /* Optional: Maximum width */
        position: relative;
        margin-left: 10px;
        text-align: left;
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light",
          "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        padding: 5px;
      }
      .margin-right-block {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light",
          "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-size: 14px;
        width: 25%; /* Change this percentage as needed */
        max-width: 256px; /* Optional: Maximum width */
        position: relative;
        text-align: left;
        padding: 10px; /* Optional: Adds padding inside the caption */
      }

      img {
        max-width: 100%; /* Make sure it fits inside the container */
        height: auto;
        display: block;
        margin: auto;
      }
      .my-video {
        max-width: 100%; /* Make sure it fits inside the container */
        height: auto;
        display: block;
        margin: auto;
      }
      /* Hide both video displays initially, will display based on JS detection */
      .vid-mobile,
      .vid-non-mobile {
        display: none;
      }

      /* Show the video content by default on non-mobile devices */
      .show-vid-mobile .vid-mobile {
        display: block;
      }
      .show-vid-non-mobile .vid-non-mobile {
        display: block;
      }

      a:link,
      a:visited {
        color: #0e7862; /*#1367a7;*/
        text-decoration: none;
      }
      a:hover {
        color: #24b597; /*#208799;*/
      }

      h1 {
        font-size: 18px;
        margin-top: 4px;
        margin-bottom: 10px;
      }

      h2 {
        font-size: 16px;
        margin-top: 4px;
        margin-bottom: 8px;
      }

      table.header {
        font-weight: 300;
        font-size: 17px;
        flex-grow: 1;
        width: 70%;
        max-width: calc(
          100% - 290px
        ); /* Adjust according to the width of .paper-code-tab */
      }
      table td,
      table td * {
        vertical-align: middle;
        position: relative;
      }
      table.paper-code-tab {
        flex-shrink: 0;
        margin-left: 8px;
        margin-top: 8px;
        padding: 0px 0px 0px 8px;
        width: 290px;
        height: 150px;
      }

      .layered-paper {
        /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow: 0px 0px 1px 1px rgba(0, 0, 0, 0.35),
          /* The top layer shadow */ 5px 5px 0 0px #fff,
          /* The second layer */ 5px 5px 1px 1px rgba(0, 0, 0, 0.35),
          /* The second layer shadow */ 10px 10px 0 0px #fff,
          /* The third layer */ 10px 10px 1px 1px rgba(0, 0, 0, 0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
      }

      hr {
        height: 1px; /* Sets the height of the line to 1 pixel */
        border: none; /* Removes the default border */
        background-color: #ddd; /* Sets the line color to black */
      }

      div.hypothesis {
        width: 80%;
        background-color: #eee;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
        font-family: Courier;
        font-size: 18px;
        text-align: center;
        margin: auto;
        padding: 16px 16px 16px 16px;
      }

      div.citation {
        font-size: 0.8em;
        background-color: #fff;
        padding: 10px;
        height: 200px;
      }

      .fade-in-inline {
        position: absolute;
        text-align: center;
        margin: auto;
        -webkit-mask-image: linear-gradient(
          to right,
          transparent 0%,
          transparent 40%,
          black 50%,
          black 90%,
          transparent 100%
        );
        mask-image: linear-gradient(
          to right,
          transparent 0%,
          transparent 40%,
          black 50%,
          black 90%,
          transparent 100%
        );
        -webkit-mask-size: 8000% 100%;
        mask-size: 8000% 100%;
        animation-name: sweepMask;
        animation-duration: 4s;
        animation-iteration-count: infinite;
        animation-timing-function: linear;
        animation-delay: -1s;
      }

      .fade-in2-inline {
        animation-delay: 1s;
      }

      .inline-div {
        position: relative;
        display: inline-block; /* Makes both the div and paragraph inline-block elements */
        vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
        width: 50px; /* Optional: Adds space between the div and the paragraph */
      }
    </style>

    <title>The Platonic Representation Hypothesis</title>
    <meta
      property="og:title"
      content="The Platonic Representation Hypothesis"
    />
    <meta charset="UTF-8" />
  </head>

  <body>
    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <table class="header" align="left">
          <tr>
            <td colspan="4">
              <span
                style="
                  font-size: 32px;
                  font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */
                "
                >Multimodal Scene Representation Learning for Spatial and
                Temporal Understanding in Video</span
              >
            </td>
          </tr>
          <tr>
            <td align="left">
              <span style="font-size: 17px"
                ><a href="your_website">Deniz Erus</a></span
              >
            </td>
            <td align="left">
              <span style="font-size: 17px"
                ><a href="your_partner's_website">Yijiang Liu</a></span
              >
            </td>
            <td align="left">
              <span style="font-size: 17px"
                ><a href="your_partner's_website">Qilmeg Doudatcz</a></span
              >
            </td>
          </tr>

          <tr>
            <td colspan="4" align="left">
              <span style="font-size: 18px">Final project for 6.7960, MIT</span>
            </td>
          </tr>
        </table>
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="intro">
      <div class="margin-left-block">
        <!-- table of contents here -->
        <div style="position: fixed; max-width: inherit; top: max(20%, 120px)">
          <b style="font-size: 16px">Outline</b><br /><br />
          <a href="#intro">Introduction</a><br /><br />
          <a href="#related_work">Related Works</a><br /><br />
          <a href="#model_architecture">Model Architecture</a><br /><br />
          <a href="#data">Data and Training Setup</a><br /><br />
          <a href="#results">Results and Discussion</a><br /><br />
          <a href="#discussion">Conclusion</a><br /><br />
          <a href="#implications_and_limitations"
            >Implications and Limitations</a
          ><br /><br />
          <a href="#citations">References</a><br /><br />
        </div>
      </div>
      <div class="main-content-block">
        <!--You can embed an image like this:-->
        <img src="./images/Teaser.png" width="512px" />
      </div>
      <div class="margin-right-block">Teaser image.</div>
    </div>

    <div class="content-margin-container" id="intro">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Introduction</h1>
        In films and short videos with narrative content, editing is a common
        tool used by directors to highlight key moments and guide the viewer’s
        attention. However, compared with the real physical world, editing
        weakens the continuity of time and space in the “story world”. A
        character may instantly appear in a far-away location in the next shot,
        and long actions may be compressed or skipped entirely. Filmmakers use
        these media to guide the viewer’s attention and construct a directed
        encounter with space, yet architects do not assume that visitors move
        through environments with such fixed viewpoints. This raises an
        important question. Can a model learn a representation of video that
        reflects the identity of a space across many views, lighting conditions,
        and points in time?<br /><br />

        Our project addresses this question by learning multimodal scene
        representations that can tell when different clips refer to the same
        physical environment. Rather than building a full continuity-checking
        system, for the scope of this project, we focus on how video and text
        features organize scenes in an embedding space and how this structure
        changes under multimodal supervision.<br /><br />

        We combine visual embeddings from a VideoMAE encoder with text-based
        features from Qwen2-VL and study how these signals shape the geometry of
        the video embedding space. We use low-dimensional projections and
        similarity matrices to see how scenes organize by location and how they
        recur across an entire film. This is a key step in identifying
        continuity concerns such as lighting inconsistencies or shifts in
        temporal order.<br /><br />

        Within this goal, we explore how much spatial structure is already
        present in a self-supervised video model and how much additional signal
        emerges through multimodal supervision. To do this, we compare three
        settings on the same videos:<br /><br />

        <ul>
          <li>
            <b>Unsupervised baseline</b>: treat VideoMAE as a frozen feature
            extractor and project segment embeddings into 2D using principal
            component analysis (PCA). We then inspect whether the resulting
            geometry implicitly groups scenes by semantic locations such as
            kitchen, living room, or exterior.
          </li>
          <li>
            <b>Teacher supervised variant</b>: use a stronger multimodal model,
            Qwen2-VL, as a teacher to assign soft scene labels, and train a
            lightweight classifier head on top of VideoMAE to predict these
            labels from video alone.
          </li>
          <li>
            <b>Text guided semantic distance</b>: embed scene metadata with a
            language model to obtain text similarities between segments, and use
            these similarities as soft weights in a contrastive loss that
            fine-tunes the VideoMAE embeddings toward a more location-aware
            space.
          </li>
        </ul>

        By contrasting these methods, we examine the degree to which a
        self-supervised video model can recover spatial scene categories without
        human labels and how much its representation improves when guided by
        semantic structure from an external teacher.<br /><br />
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="related_work">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Related Works</h1>
        <h2>Video representation learning</h2>
        Recent work in video understanding has shifted from convolution based
        models toward transformer based architectures that process spacetime
        information more directly. VideoMAE extends masked autoencoding to video
        by removing a very high portion of spacetime patches during training and
        asking the encoder to reconstruct the missing content. Let the video be
        represented as a sequence of patch tokens
        <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mrow>
            <mi>x</mi>
            <mo>&#x2208;</mo>
            <msup>
              <mi>&#x211D;</mi>
              <mrow>
                <mi>T</mi>
                <mo>&#x00D7;</mo>
                <mi>N</mi>
                <mo>&#x00D7;</mo>
                <mi>D</mi>
              </mrow>
            </msup>
          </mrow>
        </math>
        . A masking operator M selects a small visible subset, and the encoder E
        processes only these tokens to produce a latent representation
        <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mrow>
            <mi>z</mi>
            <mo>=</mo>
            <mi>E</mi>
            <mo>(</mo>
            <mi>M</mi>
            <mo>(</mo><mi>x</mi><mo>)</mo>
            <mo>)</mo>
          </mrow>
        </math>
        . The decoder D then attempts to reconstruct the original patches
        <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mrow>
            <mover>
              <mi>x</mi>
              <mo>^</mo>
            </mover>
            <mo>=</mo>
            <mi>D</mi>
            <mo>(</mo><mi>z</mi><mo>)</mo>
          </mrow>
        </math>
        . This forces the encoder to learn stable spatial cues, global
        structure, and temporal relationships rather than action specific
        features. These properties make VideoMAE suitable for identifying
        consistent spatial identity across scenes.<br /><br />

        <h2>Vision language modeling</h2>
        Language based models have become essential in video understanding,
        since narrative cues can often clarify spatial or temporal context that
        is not visually explicit. Qwen2-VL provides strong language
        representations and can operate in vision language settings that support
        reasoning over frames and subtitles. In this project, Qwen2-VL is used
        to convert video segments into dense text embeddings. These embeddings
        provide contextual signals about location, time, or narrative intent,
        which stabilizes scene identity when visual content varies due to
        lighting or framing.<br /><br />

        <h2>Multimodal fusion for scene understanding</h2>
        Combining visual and language information has become common in tasks
        such as retrieval, grounding, and cross modal alignment. Multimodal
        fusion allows models to integrate appearance features with narrative
        cues that do not appear in the raw frames. In this project, the
        normalized visual embedding v from VideoMAE and the subtitle embedding t
        from Qwen are concatenated to form a joint representation
        <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mrow>
            <mi>f</mi>
            <mo>=</mo>
            <mo>[</mo>
            <mi>v</mi>
            <mo>,</mo>
            <mi>t</mi>
            <mo>]</mo>
          </mrow>
        </math>
        . This fused representation captures both spatial structure and
        narrative meaning. Although multimodal fusion is well established in
        video question answering and video search, it has not been widely
        applied to identifying recurring architectural spaces across scenes.<br /><br />

        In practice, the main multimodal interaction in our experiments happens
        through the contrastive loss described in Section 3.3, where
        text-derived similarities act as soft weights that pull or push pairs of
        visual embeddings. This lets the language model act as an external model
        that shapes the geometry of the video embedding space.<br /><br />
      </div>

      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="model_architecture">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Model Architecture</h1>
        <img src="./images/ModelArchitecture2.png" width="800px" />
        Our model builds upon a VideoMAE-based visual backbone and extends it
        with a scene-level embedding head and a text-guided contrastive learning
        module. The architecture is designed to combine visual understanding,
        scene segmentation, and world-model reasoning within a unified
        framework. Thus, the proposed architecture for semantic distance
        understanding contains three components:
        <strong
          >Understanding Backbone(VideoMAE) + Scene Embedding Head + LLM-based
          Semantic Distance Module (Contrastive Layer).</strong
        >
        <br /><br />

        <h2>Video Understanding Backbone (VideoMAE)</h2>
        The base of our system is the pretrained MCG-NJU/videomae-base
        transformer encoder. We load a fine-tuned checkpoint that has already
        been trained on the FineVideo dataset for video understanding tasks. To
        adapt this backbone for scene-level semantic modeling, we support two
        kinds of supervisory signals: Frame-level labels, where each frame
        receives an individual label; and Scene-level shared labels, where all
        frames within a timestamp interval share one scene label. These two
        settings allow the model to shift from coarse video understanding toward
        the more structured task of scene segmentation.<br /><br />

        During fine-tuning, only the last K layers of VideoMAE’s transformer
        encoder (K = 2–4) are unfrozen. This preserves low-level visual
        representations while allowing high-level scene semantics to adapt to
        the downstream objective.<br /><br />

        <h2>Scene Embedding Head (MLP)</h2>
        To produce a scene-level representation, we append a lightweight MLP
        head to the output CLS token: MLP (“Linear”→"ReLU"→"Linear"). The MLP
        compresses the high-dimensional VideoMAE features into a compact
        embedding space in which scene relationships can be reasoned about. This
        embedding space is used both for segmentation tasks and for the semantic
        distance learning described below.<br /><br />

        <h2>Semantic Distance (LLM based) Contrastive training</h2>
        While VideoMAE provides strong visual features, it lacks world knowledge
        needed for higher-level scene reasoning.Film editing often omits
        intermediate transitions because humans can infer continuity from
        common-sense reasoning, and some videos take place in fictional
        universes where everyday expectations do not apply. This requires
        grounding in the actual visual content rather than generic knowledge. To
        address both limitations, we introduce a language-model-powered semantic
        distance module that uses text as an external world model.<br /><br />

        For each scene, we extract metadata descriptions (scene description,
        props, context) and encode them using a large language model to obtain a
        normalized embedding <i>t</i><sub>i</sub> for scene <i>i</i>. Semantic
        relatedness between two scenes <i>i</i> and <i>j</i> is measured as
        cosine similarity<br /><br />

        <center>
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <msub
                ><mi>s</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub
              >
              <mo>=</mo>
              <mi>cos</mi>
              <mo>(</mo>
              <msub><mi>t</mi><mi>i</mi></msub>
              <mo>,</mo>
              <msub><mi>t</mi><mi>j</mi></msub>
              <mo>)</mo>
            </mrow>
          </math>
        </center>
        <br />

        These embeddings provide semantic relationships that reflect narrative
        continuity, spatial logic, and object-level reasoning.<br /><br />
        We construct soft supervision weights from <i>s</i><sub>ij</sub>, <br />
        <br />
        <center>
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <msubsup>
                <mi>w</mi>
                <mrow><mi>i</mi><mi>j</mi></mrow>
                <mi>pos</mi>
              </msubsup>
              <mo>=</mo>
              <mi>f</mi>
              <mo>(</mo>
              <msub
                ><mi>s</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub
              >
              <mo>)</mo>
              <mo>,</mo>
              <mo>&#160;</mo>
              <msubsup>
                <mi>w</mi>
                <mrow><mi>i</mi><mi>j</mi></mrow>
                <mi>neg</mi>
              </msubsup>
              <mo>=</mo>
              <mi>f</mi>
              <mo>(</mo>
              <msub
                ><mi>s</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub
              >
              <mo>)</mo>
            </mrow>
          </math>
        </center>
        <br />
        where <i>w</i><sup>pos</sup><sub>ij</sub> controls how strongly we pull
        a pair together and <i>w</i><sup>neg</sup><sub>ij</sub> controls how
        strongly we push it apart. We experiment with two implementations for
        <i>f</i> and <i>g</i>:<br /><br />

        Linear mapping:<br /><br />
        <center>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mrow>
              <msub><mi>f</mi><mi>lin</mi></msub>
              <mo>(</mo><mi>s</mi><mo>)</mo> <mo>=</mo><mi>s</mi>

              <mo>,</mo>
              <mo>&#160;</mo>

              <msub><mi>g</mi><mi>lin</mi></msub>
              <mo>(</mo><mi>s</mi><mo>)</mo>
              <mo>=</mo>
              <mn>1</mn><mo>&#x2212;</mo><mi>s</mi>
            </mrow>
          </math>
        </center>
        <br />
        Squared mapping:<br /><br />
        <center>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mrow>
              <msub><mi>f</mi><mi>sq</mi></msub>
              <mo>(</mo><mi>s</mi><mo>)</mo>
              <mo>=</mo>
              <msup><mi>s</mi><mo>²</mo></msup>

              <mo>,</mo>
              <mo>&#160;</mo>

              <msub><mi>g</mi><mi>sq</mi></msub>
              <mo>(</mo><mi>s</mi><mo>)</mo>
              <mo>=</mo>
              <msup>
                <mrow>
                  <mo>(</mo><mn>1</mn><mo>&#x2212;</mo><mi>s</mi><mo>)</mo>
                </mrow>
                <mo>²</mo>
              </msup>
            </mrow>
          </math>
        </center>

        <br />

        In both cases, high text similarity leads to a strong pull, low
        similarity leads to a strong push, and mid-range similarities receive
        relatively small weights. Squaring enhances this effect by emphasizing
        very similar or very dissimilar pairs.<br /><br />

        To achieve this, first, given the video embeddings <i>v</i
        ><sub>i</sub> from the MLP head, we normalize them:<br /><br />

        <center>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mstyle displaystyle="true">
              <mrow>
                <msub>
                  <mover>
                    <mi>v</mi>
                    <mo>^</mo>
                  </mover>
                  <mi>i</mi>
                </msub>
                <mo>=</mo>
                <mfrac>
                  <msub><mi>v</mi><mi>i</mi></msub>
                  <mrow>
                    <mo>&#x2225;</mo>
                    <msub><mi>v</mi><mi>i</mi></msub>
                    <mo>&#x2225;</mo>
                  </mrow>
                </mfrac>
              </mrow>
            </mstyle>
          </math>
        </center>
        <br />

        and compute video-space similarity:<br /><br />

        <center>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mrow>
              <msubsup>
                <mi>sim</mi>
                <mrow><mi>i</mi><mi>j</mi></mrow>
                <mi>video</mi>
              </msubsup>
              <mo>=</mo>
              <msup>
                <msub>
                  <mover>
                    <mi>v</mi>
                    <mo>^</mo>
                  </mover>
                  <mi>i</mi>
                </msub>
                <mi>T</mi>
              </msup>
              <mo>&#x22C5;</mo>
              <msub>
                <mover>
                  <mi>v</mi>
                  <mo>^</mo>
                </mover>
                <mi>j</mi>
              </msub>
            </mrow>
          </math>
        </center>
        <br />

        Loss of Positive term (pulling):<br /><br />

        <center>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mrow>
              <msub><mi>L</mi><mi>pos</mi></msub>
              <mo>=</mo>
              <mi>E</mi><mo>[</mo>
              <msup><mi>w</mi><mi>pos</mi></msup>
              <mo>(</mo>
              <mn>1</mn><mo>&#x2212;</mo>
              <msubsup>
                <mi>sim</mi>
                <mrow><mi>i</mi><mi>j</mi></mrow>
                <mi>video</mi>
              </msubsup>
              <mo>)</mo>
              <mo>]</mo>
            </mrow>
          </math>
        </center>
        <br />

        Loss of Negative term (pushing):<br /><br />

        <center>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mrow>
              <msub><mi>L</mi><mi>neg</mi></msub>
              <mo>=</mo>
              <mi>E</mi><mo>[</mo>
              <msub><mi>w</mi><mi>neg</mi></msub>
              <mo>&#x22C5;</mo>
              <mi>max</mi>
              <mo>(</mo>
              <msubsup>
                <mi>sim</mi>
                <mrow><mi>i</mi><mi>j</mi></mrow>
                <mi>video</mi>
              </msubsup>
              <mo>&#x2212;</mo>
              <mi>m</mi>
              <mo>,</mo>
              <mn>0</mn>
              <mo>)</mo>
              <mo>]</mo>
            </mrow>
          </math>
        </center>
        <br />

        And then we have the final objective:<br /><br />

        <center>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mrow>
              <mi>L</mi>
              <mo>=</mo>
              <msub><mi>L</mi><mi>pos</mi></msub>
              <mo>+</mo>
              <msub><mi>L</mi><mi>neg</mi></msub>
            </mrow>
          </math>
        </center>
        <br />

        This module forces the video embedding space to align with the semantic
        topology implied by the LLM, while still remaining grounded in actual
        visual evidence. In this model architecture, both video and language
        have complementary roles. A key motivation for this architecture is that
        video-based and language-based reasoning possess complementary
        strengths:<br /><br />
        VideoMAE compensates for LLM limitations: 1. Fictional or stylized
        worlds violate real-world logic; 2. Visual cues reveal spatial layout,
        lighting, actor identity, etc.<br /><br />
        LLM compensates for VideoMAE limitations:1. Editing creates
        discontinuities humans resolve through world knowledge; 2. Similar
        visual scenes may have very different narrative functions.<br /><br />
        By coupling the two with contrastive learning, the model learns an
        embedding space that reflects both visual relations and
        narrative/world-level semantic similarity. Thus, the final
        representation is capable of supporting fine-grained scene segmentation
        and semantic distance reasoning within longer video sequences.<br /><br />
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <img
          src="./images/semanticAndVideoMAEVisualDistanceOverTime.png"
          width="650px"
        />
      </div>
      <div class="margin-right-block">
        Figure: Scene distance of original VideoMAE model before training (The
        blue line shows the interpolated semantic distance, and the orange line
        shows visual distance. The red horizontal line is the semantic
        threshold. For LLM, semantic distance fluctuates strongly and often
        crosses the threshold, while for the original VideoMAE model, visual
        distance stays near zero with only small bumps.)
      </div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <img src="./images/SemanticSimilarityMatrix.png" width="650px" />
      </div>
      <div class="margin-right-block">
        Figure: Semantic similarity matrix (Almost all visual distances are
        extremely small, clustered below about 0.02. This indicates that
        VideoMAE sees the majority of clips as visually very similar. The
        spatial layout and low level appearance of the footage remain stable
        across most of the timeline.)
      </div>
    </div>

    <div class="content-margin-container" id="results">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Data and Training Setup</h1>
        <strong>Dataset</strong>: We use the FineVideo dataset from HuggingFace,
        a large-scale collection of short video clips paired with detailed
        metadata. Each entry contains: raw video frames (mp4), structured scene
        annotations, object and prop descriptions, timestamps identifying scene
        boundaries, and contextual descriptions of each clip. This dataset is
        well-suited for training both visual scene classifiers and semantic
        video embedding models, because every scene is accompanied by rich
        natural-language descriptions. <br /><br />

        <strong>Preprocessing and Scene Extraction:</strong>
        FineVideo organizes videos at the segment level and already has assigned
        labels, therefore we can treat each unique timestamp as one scene:<br /><br />

        <ul>
          <li>
            All frames under the same timestamp (sharing one label) are treated
            as belonging to the same semantic scene.
          </li>
          <li>
            We extract relevant text fields from the metadata provided in the
            dataset (scene.description, props.name, content_title,
            contextualRelevance)
          </li>
          <li>
            These text snippets are cleaned and concatenated into a single scene
            description.
          </li>
          <li>
            We compute text embeddings using a pretrained language model and
            store with each scene sample.
          </li>
          <li>
            These embeddings later provide the semantic similarity supervision
            for contrastive training.
          </li>
        </ul>

        We randomly stream and store 3000 videos, covering 29628 scene
        segments.<br /><br />

        <strong>Baseline head training:</strong> We begin with the pretrained
        VideoMAE-Base backbone and add a lightweight fully connected head to
        produce scene-level embeddings. In this stage the backbone is frozen,
        and only the head is trained. We use AdamW with a learning rate of
        <span class="math-inline">10<sup>-4</sup></span> for the head and weight
        decay <span class="math-inline">10<sup>-4</sup></span
        >. This gives us a visual baseline representation for downstream
        analysis and for the teacher-supervised classifier.<br /><br />

        <strong>Text-guided contrastive fine-tuning:</strong> After establishing
        a visual baseline and training several models with it, we introduce
        semantic supervision from text embeddings using our contrastive loss. At
        this stage, we partially unfreeze the last 2–4 transformer layers (we
        used K = 2 in early experiments, K = 4 in final experiments), the new
        learning rates for backbone (unfrozen layers) are
        <span class="math-inline">5 × 10<sup>-5</sup></span
        >, and <span class="math-inline">5 × 10<sup>-4</sup></span> for MLP
        head. During this stage, the video embeddings are trained to reflect the
        semantic distances implied by the textual descriptions. Scenes that are
        close in text space receive stronger “pull” weights, and scenes that are
        far apart receive stronger “push” weights.<br /><br />

        <strong>Teacher-supervised variant:</strong> To add explicit semantic
        structure, we also experiment with a teacher-supervised classifier. For
        each segment, we prompt Qwen2-VL with a short video clip and ask it to
        assign a location label such as “kitchen”, “living room”, or “outside”.
        These labels serve as targets for a linear classifier head on top of the
        scene embeddings. We keep the VideoMAE backbone frozen and train the
        classifier with cross-entropy loss. Comparing this teacher-supervised
        classifier to the unsupervised PCA baseline lets us qualitatively assess
        how much additional structure we gain from multimodal supervision by
        inspecting the PCA projections before and after training.<br /><br />
      </div>
      <div class="margin-right-block"></div>
    </div>
  </body>

  <div class="content-margin-container" id="discussion">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h1>Results and Discussion</h1>
    </div>
    <div class="margin-right-block"></div>
  </div>

  <div class="content-margin-container">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <img src="./images/PCA.png" width="650px" />
    </div>
    <div class="margin-right-block">
      Figure: PCA of scene embeddings before and after Qwen supervision on a
      short sample video with three locations: outside (blue), kitchen (green),
      living room (red).
    </div>
  </div>

  <div class="content-margin-container">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h2>PCA Analysis of scene embeddings</h2>
      We first analyze the unsupervised structure of VideoMAE embeddings using
      principal component analysis. For each scene, we take its CLS embedding,
      compute a 2D PCA projection, and color the point by Qwen2-VL’s location
      label.<br /><br />
      Baseline (pre-supervision): Using embeddings from the pretrained VideoMAE
      model, PCA reveals that indoor vs outdoor scenes often drift into somewhat
      different regions of the 2D plane, this is most likely due to large
      brightness difference between the scenes. However, indoor categories
      heavily overlap. Some scenes from the same physical location appear far
      apart in PCA space when the camera angle changes. This indicates that the
      pretrained model does not enforce a“place identity”.<br /><br />
      After Qwen-guided training (post-supervision): We recompute scene
      embeddings and PCA projections. The distribution changes in several ways:
      Scenes sharing the same location label form tighter groups. Different
      location categories become more separated, for example, kitchens and
      living rooms now occupy more distinct regions.<br /><br />
      The PCA plots before and after supervision give a qualitative picture of
      how the embedding space is being reshaped: from a generic representation
      dominated by low-level visual similarity to a more location-aware
      embedding where place identity plays a stronger role.<br /><br />

      <h2>Text-guided contrastive fine-tuning</h2>

      The LLM-generated embeddings allow us to compute semantic relatedness
      between scenes, which serves as the supervisory signal for video embedding
      training. Qualitative inspection confirms that the LLM is able to assign
      reasonable similarity scores based on narrative context, object
      composition, and scene semantics.<br /><br />

      <strong>Baseline VideoMAE Scene Similarity:</strong> Before introducing
      text-driven supervision, we evaluate the baseline VideoMAE model on a
      multi-scene video. The model produces extremely high similarity (0.9-1.0)
      across all scenes, regardless of actual semantic differences. This
      indicates that although VideoMAE captures visual content well, its
      embedding space lacks the semantic structure needed to distinguish
      narrative roles or scene types.<br /><br />

      <strong>Fine-tuning with Linear Text-Similarity Weights:</strong> In the
      second experiment, we introduce contrastive supervision using linearly
      mapped text similarity as positive/negative weights. We unfreeze the last
      two transformer layers and train for 2000 steps. Training loss fluctuates
      between 0.30–0.27, showing only a mild downward trend. Despite the small
      change in loss, the model meaningfully restructures its embedding space.
      The fine-tuned model succeeds in separating different categories of
      scenes:<br /><br />

      <ul>
        <li>
          Non-narrative transitional frames (e.g., fades to black, title cards):
          similarity ≈ 0.5
        </li>
        <li>
          Semantically distinct locations (indoor vs. outdoor, clearly different
          settings): similarity ≈ 0.7
        </li>
        <li>
          Highly similar narrative scenes (same characters performing the same
          action): similarity ≈ 0.95–1.0
        </li>
      </ul>

      We assume the reason why loss barely changes but the model still learns
      is: first, the loss averages over all pairwise interactions in the batch.
      Even small adjustments in embedding geometry are diluted when averaged
      across dozens or hundreds of pairs. Second, the contrastive weights are
      soft and continuous, not binary. This makes the optimization landscape
      smooth, producing subtle numerical changes even when the embedding space
      is being noticeably reorganized. Third, semantic improvements mainly
      affect relative distances, but the global magnitude of the loss may remain
      similar.<br /><br />

      <strong>Fine-tuning with Squared Text-Similarity Weights:</strong> In the
      third experiment, we modify the weighting function as<br /><br />

      <!-- THE ONLY MATHML BLOCK YOU REQUESTED -->
      <center>
        <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mrow>
            <!-- w^{pos} = s^2 -->
            <msup>
              <mi>w</mi>
              <mrow><mi>pos</mi></mrow>
            </msup>
            <mo>=</mo>
            <msup>
              <mi>s</mi>
              <mn>2</mn>
            </msup>
            <mo>,</mo>
            <!-- w^{neg} = (1 - s)^2 -->
            <msup>
              <mi>w</mi>
              <mrow><mi>neg</mi></mrow>
            </msup>
            <mo>=</mo>
            <msup>
              <mrow>
                <mo>(</mo>
                <mn>1</mn>
                <mo>&#x2212;</mo>
                <mi>s</mi>
                <mo>)</mo>
              </mrow>
              <mn>2</mn>
            </msup>
          </mrow>
        </math>
      </center>
      <br />

      This mapping suppresses mid-level weights and concentrates samples into
      strong positives (≈0.7–0.8) and strong negatives (≈0.1–0.2). We unfreeze
      four transformer layers and train for 4000 steps. Training loss stabilizes
      between 0.11–0.13, with no clear downward trend. The model shows two
      striking behaviors:<br /><br />

      <ol>
        <li>
          It very clearly separates transitional frames (fades, black frames,
          title cards). These consistently receive the lowest similarity in the
          embedding space.
        </li>
        <li>
          However, all narrative scenes collapse into an extremely tight
          cluster, producing similarity scores equal to 1 between nearly all
          meaningful scenes.
        </li>
      </ol>

      We assume the reasons are as follows: Squared weighting amplifies strong
      positives disproportionately, High text-similarity pairs dominate the
      gradient signal, pulling many narrative scenes tightly together.
      Unfreezing more layers increases model plasticity. With four layers
      updated, the model can reshape its embedding space more dramatically,
      risking representation collapse toward a single cluster for "narrative
      scenes." Last but not least, Most narrative scenes share similar objects,
      actors, or contexts. Their text embeddings form a dense cluster; after
      squaring, their weights become even more dominant.<br /><br />
    </div>
    <div class="margin-right-block"></div>
  </div>

  <div class="content-margin-container">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <img src="./images/SimilarityHeatMap.jpeg" width="650px" />
    </div>
    <div class="margin-right-block">
      Figure: Similarity heatmap of 20 scenes from the Star Wars movie
    </div>
  </div>

  <div class="content-margin-container">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h1>Conclusion</h1>
      Overall, the project suggests that a frozen, self-supervised video
      backbone already encodes some useful spatial and appearance structure. But
      it is not a stable world model of place. Multimodal supervision from an
      LLM-based teacher can significantly improve alignment between visual
      embeddings and scene identity, even when labels and text are only
      pseudo-annotations. PCA projections and similarity matrices provide a
      useful view into how these representations are organized and how they
      change with supervision.<br /><br />
    </div>
    <div class="margin-right-block"></div>
  </div>

  <div class="content-margin-container" id="implications_and_limitations">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <h1>Implications and limitations</h1>
      This project shows that self-supervised video models, especially if there
      is teacher supervision, can capture meaningful scene structure. The
      VideoMAE provides embeddings that can sometimes approximate the teacher's
      decision about whether two segments belong to the same location. This
      suggests that a good amount of spatial information is already present in
      the representation.<br /><br />
      Qwen2-VL makes it possible to annotate many segments without manual work.
      This makes it possible to train stronger heads and to fine-tune video
      transformers for scene understanding.<br /><br />
      This project also has several limitations. The experiments are limited to
      a small number of videos due to storage and compute constraints. This
      makes the conclusions more qualitative rather than solid quantitative
      results. Another limitation is that we use teacher labels as the ground
      truth. If the teacher is systematically mislabeling the scenes, the
      student will reproduce these errors and we will not be aware. A more
      rigorous solution would be to add human annotations of scene types and
      boundaries.<br /><br />
      Future work could scale this idea up to use more data, try different
      teachers and architectures, and check performance in automated continuity
      checking while editing videos.<br /><br />
    </div>
    <div class="margin-right-block"></div>
  </div>

  <div class="content-margin-container" id="citations">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
      <div class="citation" id="references" style="height: auto">
        <br />
        <span style="font-size: 16px">References:</span><br /><br />
        <a id="ref_1"></a>[1]
        <a href="https://doi.org/10.1109/CVPR52729.2023.00632">
          Movies2Scenes: Using Movie Metadata to Learn Scene Representation </a
        >, Chen, Shixing, Chun-Hao Liu, Xiang Hao, Xiaohan Nie, Maxim Arap, and
        Raffay Hamid, 2023<br /><br />

        <a id="ref_2"></a>[2]
        <a href="https://doi.org/10.1109/CVPR46437.2021.00967">
          Shot Contrastive Self-Supervised Learning for Scene Boundary Detection </a
        >, Chen, Shixing, Xiaohan Nie, David Fan, Dongqing Zhang, Vimal Bhat,
        and Raffay Hamid, 2021<br /><br />

        <a id="ref_3"></a>[3]
        <a href="https://github.com/MCG-NJU/VideoMAE/blob/main/FINETUNE.md">
          VideoMAE/FINETUNE.Md at Main · MCG-NJU/VideoMAE </a
        >, GitHub, n.d., Accessed December 1, 2025<br /><br />

        <a id="ref_4"></a>[4]
        <a href="https://movienet.github.io/?utm_source=chatgpt.com">
          MovieNet </a
        >, n.d., Accessed November 23, 2025<br /><br />

        <a id="ref_5"></a>[5]
        <a href="https://github.com/MCG-NJU/VideoMAE"> MCG-NJU/VideoMAE </a>,
        Multimedia Computing Group, Nanjing University, (2022) 2025, Python,
        March 23, Released November 22<br /><br />

        <a id="ref_6"></a>[6]
        <a href="https://doi.org/10.1609/aaai.v39i7.32773">
          Modality-Aware Shot Relating and Comparing for Video Scene Detection </a
        >, Tan, Jiawei, Hongxing Wang, Kang Dang, Jiaxin Li, and Zhilong Ou,
        2025<br /><br />

        <a id="ref_7"></a>[7]
        <a href="https://doi.org/10.48550/arXiv.2203.12602">
          VideoMAE: Masked Autoencoders Are Data-Efficient Learners for
          Self-Supervised Video Pre-Training </a
        >, Tong, Zhan, Yibing Song, Jue Wang, and Limin Wang, 2022<br /><br />

        <a id="ref_8"></a>[8]
        <a
          href="https://research.google/blog/vid2seq-a-pretrained-visual-language-model-for-describing-multi-event-videos/"
        >
          Vid2Seq: A Pretrained Visual Language Model for Describing Multi-Event
          Videos </a
        >, n.d., Accessed December 1, 2025<br /><br />

        <a id="ref_9"></a>[9]
        <a href="https://doi.org/10.48550/arXiv.2303.16727">
          VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking </a
        >, Wang, Limin, Bingkun Huang, Zhiyu Zhao, et al., 2023<br /><br />

        <a id="ref_10"></a>[10]
        <a href="https://doi.org/10.1609/aaai.v37i3.25426">
          Towards Global Video Scene Segmentation with Context-Aware Transformer </a
        >, Yang, Yang, Yurui Huang, Weili Guo, Baohua Xu, and Dingyin Xia,
        2023<br /><br />
      </div>
    </div>
    <div class="margin-right-block">
      <!-- margin notes for reference block here -->
    </div>
  </div>

  <p id="wordCount"></p>
</html>

<script>
  function countWords(text) {
    return text.trim().split(/\s+/).length;
  }

  const content = document.body.innerText;
  const totalWords = countWords(content);

  document.getElementById("wordCount").innerText = "Word count: " + totalWords;
</script>
